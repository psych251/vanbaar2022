{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Behavioral Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is copied from the repository provided by the original authors, with some edits made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob, scipy, sqlite3, json, matplotlib#, pymer4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import matplotlib.ticker as mtick\n",
    "import json\n",
    "import glob\n",
    "# import FigureTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style\n",
    "import matplotlib.style as style\n",
    "#style.use('seaborn-poster') #sets the size of the charts\n",
    "#style.use('seaborn-white')\n",
    "sns.set_palette('tab10')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['font.family'] = \"Helvetica\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Nora/Documents/Github/courses/psych251\n",
      "/Users/Nora/Documents/Github/courses/psych251/vanbaar2022\n",
      "/Users/Nora/Documents/Github/courses/psych251/vanbaar2022/data/pilotB\n"
     ]
    }
   ],
   "source": [
    "# Filepaths\n",
    "class_dir = os.path.abspath('../../')\n",
    "print(class_dir)\n",
    "proj_dir = os.path.join(class_dir,'vanbaar2022')\n",
    "print(proj_dir)\n",
    "data_dir = os.path.join(proj_dir,'data/pilotB')\n",
    "print(data_dir)\n",
    "\n",
    "# sys.path.append('/'.join(os.path.realpath('..').split('/')[:4]) + '/Python')\n",
    "# import FigureTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The csv files are ['/Users/Nora/Documents/Github/courses/psych251/vanbaar2022/data/pilotB/ys5qocn3rv_trials.csv']\n",
      "There are 1 participants\n",
      "The columns are Index(['view_history', 'rt', 'trial_type', 'trial_index', 'plugin_version',\n",
      "       'time_elapsed', 'subjectID', 'prolificID', 'studyID', 'sessionID',\n",
      "       'overallBonusPoints', 'task', 'response', 'question_order', 'success',\n",
      "       'Matrix', 'S', 'T', 'R', 'P', 'GameType', 'choice', 'GivenAns',\n",
      "       'Player', 'PlayerType', 'CorrAns', 'confidence', 'ScoreNum',\n",
      "       'time_on_trial', 'stimulus'],\n",
      "      dtype='object')\n",
      "1 participants found\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all CSV files in the folder\n",
    "all_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "print(f'The csv files are {all_files}')\n",
    "\n",
    "# Get the number of CSV files in the folder\n",
    "num_participants = len(all_files)\n",
    "print(f'There are {num_participants} participants')\n",
    "\n",
    "# Read each CSV into a DataFrame and store them in a list\n",
    "list_of_dfs = [pd.read_csv(f) for f in all_files]\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "# Examine df\n",
    "print(f'The columns are {df.columns}')\n",
    "df.head(3)\n",
    "\n",
    "# Check how many participants\n",
    "print('%i participants found'%len(df['subjectID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows which hold the responses to the survey data\n",
    "surveyDat = df[df['task'].isin(['demographics', 'technical'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unecessary columns\n",
    "cols = ['subjectID', 'studyID', 'sessionID', 'task', 'response']\n",
    "surveyDat = surveyDat[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine df\n",
    "surveyDat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Task Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows which hold the responses to the social prediction game\n",
    "taskDat = df[df['task'] == 'socialPredictionGame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unecessary columns\n",
    "cols = ['rt', 'time_elapsed', 'subjectID', 'studyID', 'sessionID', 'task', 'Matrix', 'S', 'T', 'R', 'P', 'GameType', 'choice', 'GivenAns', 'Player', 'PlayerType', 'CorrAns', 'confidence', 'ScoreNum', 'stimulus']\n",
    "taskDat = taskDat[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine df\n",
    "taskDat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to correspond with those used in paper\n",
    "taskDat.rename(columns = {\n",
    "    'subjectID': 'subID',\n",
    "    'PlayerType': 'Type_Total',\n",
    "    'confidence': 'Confidence',\n",
    "    'ScoreNum': 'Score'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# ['Type_Total', 'Type', 'Variant', 'Confidence', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'Type' and 'Variant' columns from 'Type_Total'\n",
    "taskDat[['Type', 'Variant']] = taskDat['Type_Total'].str.split('_', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# firstType = gameDat.loc[(gameDat['Trial']==0) & (gameDat['Block']==0), ['subID','Type_Total']].reset_index(drop=True)\n",
    "# firstType.columns = ['subID','FirstType']\n",
    "# thirdType = gameDat.loc[(gameDat['Trial']==0) & (gameDat['Block']==2), ['subID','Type_Total']].reset_index(drop=True)\n",
    "# thirdType.columns = ['subID','ThirdType']\n",
    "# gameDat = gameDat.merge(firstType,on='subID').merge(thirdType,on='subID')\n",
    "# gameDat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gtOrder = ['HG','SG','SH','PD']\n",
    "# roundOrder = range(4)\n",
    "# ptOrder = ['opt_nat','pess_nat','opt_inv','pess_inv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best score: subject 2133. What did they discover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gameDat.loc[gameDat['subID']==2133,['Type_Total','SelfReport']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall performance by player type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "# change to analysis code -- blockDat changed bc code wasn't working\n",
    "blockDat = (taskDat.groupby(['subID', 'Variant'], as_index=False)[['Confidence', 'Score']].mean())\n",
    "# run this line instead (no grouping by subID if running code for only one participant)\n",
    "# blockDat = (taskDat.groupby('Variant', as_index=False)[['Confidence', 'Score']].mean())\n",
    "fig, ax = plt.subplots(1,1,figsize=[6,5])\n",
    "sns.barplot(data=blockDat,x='Variant',y='Score', ax=ax, errwidth = 3, capsize=.1,\n",
    "            order=['nat','inv'],alpha=0)\n",
    "sns.swarmplot(data=blockDat,x='Variant',y='Score', ax=ax,\n",
    "            order=['nat','inv'], alpha=.3, color = 'k')\n",
    "ax.plot([-5,5],[.5,.5], 'k--', lw=2)\n",
    "ax.set(ylim = [0,1.1], xlim = [-.5,1.5], xlabel = None, yticks = [0,.25,.5,.75,1],\n",
    "       title = 'Performance by strategy type',\n",
    "       xticklabels = ['Human\\nStrategies', 'Artificial\\nStrategies'], ylabel = 'Accuracy     ');\n",
    "dat1 = blockDat.loc[blockDat['Variant']=='nat','Score'].values\n",
    "dat2 = blockDat.loc[blockDat['Variant']=='inv','Score'].values\n",
    "stats = scipy.stats.ttest_rel(dat2,dat1)\n",
    "# FigureTools.add_sig_markers(ax, relationships=[[0,1,stats[1]]])\n",
    "sns.despine(top=True,right=True)\n",
    "ax.spines['left'].set_bounds(0,1)\n",
    "ax.set_ylim([0,1.4])\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# plt.savefig(baseDir+'/Figures/plot1b.pdf',transparent=True, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "# change to analysis code -- overallDat changed bc code wasn't working\n",
    "overallDat = (taskDat.groupby(['subID'], as_index=False)[['Confidence', 'Score']].mean())\n",
    "fig, ax = plt.subplots(1,1,figsize=[6,5])\n",
    "sns.barplot(data=blockDat,y='Score', ax=ax, errwidth = 3, capsize=.1,\n",
    "            alpha=0)\n",
    "sns.swarmplot(data=blockDat,y='Score', ax=ax,\n",
    "            alpha=.3, color = 'k')\n",
    "ax.plot([-5,5],[.5,.5], 'k--', lw=2)\n",
    "ax.set(ylim = [0,1.1], xlabel = None, yticks = [0,.25,.5,.75,1],\n",
    "       title = 'Overall task performance',\n",
    "       ylabel = 'Accuracy        ');\n",
    "stats = scipy.stats.ttest_1samp(overallDat['Score'].values, 0.5)\n",
    "# FigureTools.add_sig_markers(ax, relationships=[[0,0,stats[1]]])\n",
    "sns.despine(top=True,right=True)\n",
    "ax.spines['left'].set_bounds(0,1)\n",
    "ax.set_ylim([0,1.4])\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# plt.savefig(baseDir+'/Figures/plot1d.pdf',transparent=True, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_1samp(dat, popmean = .5, verbose = True):\n",
    "    if verbose:\n",
    "        print('SD: ',np.std(dat))\n",
    "        print('Stats:')\n",
    "    stats = scipy.stats.ttest_1samp(dat, popmean = popmean)\n",
    "    if verbose:\n",
    "        print(stats)\n",
    "        print('Cohen d:')\n",
    "    coh_d = (np.mean(dat) - popmean)/np.std(dat)\n",
    "    if verbose:\n",
    "        print(coh_d)\n",
    "    return(stats, coh_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_2samp(dat1, dat2):\n",
    "    print('SD dat 1: ',np.std(dat1))\n",
    "    print('SD dat 2: ',np.std(dat2))\n",
    "    print('Stats:')\n",
    "    stats = scipy.stats.ttest_ind(dat1, dat2)\n",
    "    print(stats)\n",
    "    print('Cohen d:')\n",
    "    nx = len(dat1)\n",
    "    ny = len(dat2)\n",
    "    dof = nx + ny - 2\n",
    "    coh_d = ((np.mean(dat1) - np.mean(dat2)) /\n",
    "             np.sqrt(((nx-1)*np.std(dat1, ddof=1) ** 2 + (ny-1)*np.std(y, ddof=1) ** 2) / dof))\n",
    "    print(coh_d)\n",
    "    return(stats, coh_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_1samp(overallDat['Score'].values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.wilcoxon(overallDat['Score'].values-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot model reproduction of this effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dat = pd.read_csv(baseDir + '/Data/Cleaned/Model_simulations_%s_%s.csv'%('CoGrRiNa','best'), index_col=0)\n",
    "sim_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "sim_block_dat = sim_dat[['subID','Type_Total','Variant','Confidence','model_score']\n",
    "                  ].groupby(['subID','Variant']).mean().reset_index()\n",
    "fig, ax = plt.subplots(1,1,figsize=[6,5])\n",
    "sns.barplot(data=sim_block_dat,x='Variant',y='model_score', ax=ax, errwidth = 3, capsize=.1,\n",
    "            order=['nat','inv'],alpha=0)\n",
    "sns.swarmplot(data=sim_block_dat,x='Variant',y='model_score', ax=ax,\n",
    "            order=['nat','inv'], alpha=.3, color = 'k')\n",
    "ax.plot([-5,5],[.5,.5], 'k--', lw=2)\n",
    "ax.set(ylim = [0,1.1], xlim = [-.5,1.5], xlabel = None, yticks = [0,.25,.5,.75,1],\n",
    "       title = 'Model prediction',\n",
    "       xticklabels = ['Human\\nStrategies', 'Artificial\\nStrategies'], ylabel = 'Accuracy     ');\n",
    "dat1 = sim_block_dat.loc[sim_block_dat['Variant']=='nat','model_score'].values\n",
    "dat2 = sim_block_dat.loc[sim_block_dat['Variant']=='inv','model_score'].values\n",
    "stats = scipy.stats.ttest_rel(dat2,dat1)\n",
    "FigureTools.add_sig_markers(ax, relationships=[[0,1,stats[1]]])\n",
    "sns.despine(top=True,right=True)\n",
    "ax.spines['left'].set_bounds(0,1)\n",
    "ax.set_ylim([0,1.4])\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# plt.savefig(baseDir+'/Figures/plot1c.pdf',transparent=True, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean performance, compare distribution of mean per subject against 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskDat['Score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanPerSub = taskDat.groupby('subID').mean()['Score'].values\n",
    "print(len(meanPerSub))\n",
    "scipy.stats.ttest_1samp(meanPerSub, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare human vs artificial strategy performance using within subjects t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified bc of error (agg function failed [how->mean,dtype->object])\n",
    "meanPerSubCondition = taskDat.groupby(['subID','Variant'], as_index=False)['Score'].mean().pivot(\n",
    "    index='subID', columns='Variant', values='Score')\n",
    "meanPerSubCondition.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanPerSubCondition['nat'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_1samp(meanPerSubCondition['nat'], .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanPerSubCondition['inv'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_1samp(meanPerSubCondition['inv'], .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between natural and artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_rel(meanPerSubCondition['inv'],meanPerSubCondition['nat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_1samp((meanPerSubCondition['nat']-meanPerSubCondition['inv']), popmean = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
